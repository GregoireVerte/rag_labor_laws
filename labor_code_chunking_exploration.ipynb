{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a18aa5-0489-4a8f-9e33-fb49cad6081a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Znaleziono 477 jednostek redakcyjnych (artykułów).\n",
      "\n",
      "Przykład artykułu 1 (Art. 1):\n",
      "U S T AWA \n",
      "z dnia 26 czerwca 1974 r. \n",
      "Kodeks pracy1) \n",
      "Preambuła (uchylona) \n",
      " \n",
      "1) Niniejsza ustawa dokonuje w  zakresie swojej regulacji wdrożenia następujących dyrektyw \n",
      "Wspólnot Europejskich: \n",
      "1) dyrektywy 83/477/EWG z dnia 19 września 1983 r. w sprawie ochrony pracowników przed \n",
      "ryzykiem związanym\n",
      "\n",
      "Przykład artykułu ze środka:\n",
      "Art. 612. § 1. Odszkodowanie, o  którym mowa w  art. 611, przysługuje \n",
      "w wysokości wynagrodzenia pracownika za okres wypowiedzenia. W  przypadku \n",
      "rozwiązania umowy o  pracę zawartej na czas określony, odszkodowanie \n",
      "przysługuje w wysokości wynagrodzenia za czas, do którego umowa miała trwać, \n",
      "nie wi\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 1. Wczytanie dokumentu\n",
    "file_path = \"last_unified_labor_code.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load()\n",
    "\n",
    "# 2. Łączenie wszystkich stron w jeden tekst i proste czyszczenie szumu\n",
    "full_text = \"\"\n",
    "for page in pages:\n",
    "    content = page.page_content\n",
    "    # Usuwanie stopki (©Kancelaria Sejmu + data + numer strony)\n",
    "    # Regex dopasuje np. \"©Kancelaria Sejmu s. 5/185\" oraz datę pod spodem\n",
    "    content = re.sub(r\"©Kancelaria Sejmu.*s\\.\\s\\d+/\\d+\", \"\", content)\n",
    "    content = re.sub(r\"2026-02-03\", \"\", content) # usuwa datę generowania\n",
    "    full_text += content + \"\\n\"\n",
    "\n",
    "# 3. Funkcja do podziału na artykuły\n",
    "def split_into_articles(text):\n",
    "    # Szuka wzorca: \"Art. [liczba i ewentualnie litera].\"\n",
    "    # Używa (?=Art\\.) aby zachować \"Art.\" na początku każdego chunka\n",
    "    pattern = r\"(?=Art\\.\\s+\\d+[a-z]*\\.)\"\n",
    "    chunks = re.split(pattern, text)\n",
    "    \n",
    "    # Usuwa ewentualny pusty pierwszy element i białe znaki\n",
    "    chunks = [c.strip() for c in chunks if c.strip()]\n",
    "    return chunks\n",
    "\n",
    "articles = split_into_articles(full_text)\n",
    "\n",
    "# podgląd\n",
    "print(f\"Znaleziono {len(articles)} jednostek redakcyjnych (artykułów).\")\n",
    "print(\"\\nPrzykład artykułu 1 (Art. 1):\")\n",
    "print(articles[0][:300])\n",
    "print(\"\\nPrzykład artykułu ze środka:\")\n",
    "print(articles[100][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478d0d24-de39-40ce-8b1d-115d8b1e6745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przygotowano 477 obiektów z metadanymi.\n",
      "Przykładowy wpis (indeks 5):\n",
      "Metadane: {'art_id': 'Art. 4', 'source': 'Kodeks Pracy', 'status_date': '2026-02-03', 'chunk_id': 5}\n",
      "Treść (początek): Art. 4. (uchylony)...\n"
     ]
    }
   ],
   "source": [
    "processed_data = []\n",
    "\n",
    "for i, chunk in enumerate(articles):\n",
    "    # 1. Wyciąga numer artykułu z początku tekstu (np. \"Art. 1.\") ## szuka wzorca na samym początku chunka\n",
    "    match = re.search(r\"Art\\.\\s+(\\d+[a-z]*)\", chunk)\n",
    "    \n",
    "    if match:\n",
    "        art_number = f\"Art. {match.group(1)}\"\n",
    "    else:\n",
    "        # jeśli to pierwszy chunk (ten z tytułem i preambułą)\n",
    "        art_number = \"Wstęp/Tytuł\"\n",
    "\n",
    "    # 2. Tworzy słownik z metadanymi\n",
    "    metadata = {\n",
    "        \"art_id\": art_number,\n",
    "        \"source\": \"Kodeks Pracy\",\n",
    "        \"status_date\": \"2026-02-03\",\n",
    "        \"chunk_id\": i\n",
    "    }\n",
    "    \n",
    "    # 3. Dodajemy do listy gotowy obiekt\n",
    "    processed_data.append({\n",
    "        \"content\": chunk,\n",
    "        \"metadata\": metadata\n",
    "    })\n",
    "\n",
    "# test\n",
    "print(f\"Przygotowano {len(processed_data)} obiektów z metadanymi.\")\n",
    "print(f\"Przykładowy wpis (indeks 5):\")\n",
    "print(f\"Metadane: {processed_data[5]['metadata']}\")\n",
    "print(f\"Treść (początek): {processed_data[5]['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a55fde3-79fa-4cd9-a057-a765b2d7523b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From O:\\Projects\\fashion_mnist\\pyth311env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Używam urządzenia: CUDA\n",
      "\n",
      "Model załadowany: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Rozmiar wektora (wymiary): 384\n",
      "Przykładowe wartości (pierwsze 5): [-0.06652039  0.18819958 -0.33668807  0.27820015  0.07411028]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# 1. Wybór modelu (multilingual, 384 wymiary)\n",
    "model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "# Sprawdza czy CUDA (GPU) jest dostępne\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Używam urządzenia: {device.upper()}\")\n",
    "\n",
    "# 2. Ładowanie modelu\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "# 3. TEST: Embeddings Sample (na pierwszym artykule)\n",
    "sample_text = processed_data[1]['content'] ### Art. 1\n",
    "sample_vector = model.encode(sample_text)\n",
    "\n",
    "print(f\"\\nModel załadowany: {model_name}\")\n",
    "print(f\"Rozmiar wektora (wymiary): {len(sample_vector)}\")\n",
    "print(f\"Przykładowe wartości (pierwsze 5): {sample_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ff8b2-f44f-4ab8-921e-0cce7e349a21",
   "metadata": {},
   "source": [
    "# Full Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32269cc-b0d5-40eb-aeda-b70466cae91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generowanie wektorów dla 477 artykułów...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e707d04a00bb425ca7536b59f53ad167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wszystkie artykuły zostały zamienione na wektory.\n"
     ]
    }
   ],
   "source": [
    "# 4. Wektoryzacja całego zbioru (full)\n",
    "print(f\"Generowanie wektorów dla {len(processed_data)} artykułów...\")\n",
    "\n",
    "# Wyciąga samą treść do zakodowania\n",
    "texts_to_encode = [item['content'] for item in processed_data]\n",
    "\n",
    "# Generuje wektory\n",
    "all_embeddings = model.encode(texts_to_encode, show_progress_bar=True)\n",
    "\n",
    "# Dodaje wektory do struktury danych\n",
    "for i, item in enumerate(processed_data):\n",
    "    item['embedding'] = all_embeddings[i].tolist() ### zmienia na listę żeby Qdrant to przyjął\n",
    "\n",
    "print(f\"Wszystkie artykuły zostały zamienione na wektory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd47a09b-3263-40ae-9a03-e05d49bf3208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sprawdzanie wpisu o indeksie 15:\n",
      "Metadane: {'art_id': 'Art. 112', 'source': 'Kodeks Pracy', 'status_date': '2026-02-03', 'chunk_id': 15}\n",
      "Treść (fragment): Art. 112. Pracownicy mają równe prawa z  tytułu jednakowego wypełniania \n",
      "takich samych obowiązków; dotyczy to w  szczególności równego traktowania \n",
      "mę...\n",
      "Rozmiar wektora: 384\n",
      "Wektor (początek): [-0.00595830800011754, 0.29976141452789307, -0.24657131731510162, -0.023757465183734894, 0.1003313809633255]...\n"
     ]
    }
   ],
   "source": [
    "#### próbka kontrolna\n",
    "idx = 15\n",
    "print(f\"Sprawdzanie wpisu o indeksie {idx}:\")\n",
    "print(f\"Metadane: {processed_data[idx]['metadata']}\")\n",
    "print(f\"Treść (fragment): {processed_data[idx]['content'][:150]}...\")\n",
    "print(f\"Rozmiar wektora: {len(processed_data[idx]['embedding'])}\")\n",
    "print(f\"Wektor (początek): {processed_data[idx]['embedding'][:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "506ad862-f23f-4ead-8a95-b346e81cd35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Greg Z\\AppData\\Local\\Temp\\ipykernel_34512\\3236154716.py:5: UserWarning: Qdrant client version 1.13.0 is incompatible with server version 1.16.3. Major versions should match and minor version difference must not exceed 1. Set check_version=False to skip version check.\n",
      "  client = QdrantClient(host=\"localhost\", port=6333)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tworzenie kolekcji: labor_code_pl\n",
      "Wysyłanie 477 punktów do Qdranta...\n",
      "Zakończono.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "# 1. Połączenie z Qdrantem\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "COLLECTION_NAME = \"labor_code_pl\"\n",
    "\n",
    "# 2. Tworzenie kolekcji (jeśli nie istnieje)\n",
    "if not client.collection_exists(collection_name=COLLECTION_NAME):\n",
    "    print(f\"Tworzenie kolekcji: {COLLECTION_NAME}\")\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    "    )\n",
    "else:\n",
    "    print(f\"Kolekcja '{COLLECTION_NAME}' już istnieje.\")\n",
    "\n",
    "# 3. Przygotowanie \"Punktów\" (Points) do wysłania\n",
    "points = []\n",
    "for i, item in enumerate(processed_data):\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=i, \n",
    "            vector=item['embedding'], \n",
    "            payload={\n",
    "                \"content\": item['content'],\n",
    "                **item['metadata'] ## rozpakowuje metadane (art_id, source itp.)\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 4. Wysyłka (Upsert)\n",
    "print(f\"Wysyłanie {len(points)} punktów do Qdranta...\")\n",
    "client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "print(\"Zakończono.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce81351-c7ec-4234-98d7-69b0228a4afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311env",
   "language": "python",
   "name": "pyth311env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
